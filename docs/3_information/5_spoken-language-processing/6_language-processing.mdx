---
title: 6. 言語処理
---
:::warning[目標]
文章（ = 文字列）から何らかの意味表現を取り出す処理。ここでは，ニューラルネットワークを用いた処理には触れない。
:::
1. 形態素解析：文字列を単語や形態素などの構成要素に分割，品詞の同定
2. 構造解析：修飾関係同定
3. 意味解析：文中の動詞を中心として文の意味的解釈を行う
4. 文脈解析：代名詞の指示先同定，文同士の論理関係同定
---
## 形態素解析
:::warning[目標]
文字列を単語や形態素などの構成要素に分割し，品詞の同定を行う。
:::
- 音声認識の最初の段階などに使われる
:::tip[形態素]
形態素：自然言語の文において，1 つ以上の音素の並びにより， **意味** を担う最小の言語単位。日本語なら基本的に単語に相当する。
:::
- IPA が品詞体系を定めている

### 解析アルゴリズム
1. 初期化：ポインタを文頭に置く
2. 辞書検索：ポインタ一位置から辞書検索
3. 連接チェック：連接可能なもの全てにエッジをはる = 辞書検索であった文字に木を作るイメージ
4. ポインターを移動する：ポインターは最も短い文字の次の位置に移動
5. ポインタが文末に到達するまで繰り返す = 左から右へ進むネットワーク的なものができる

実際にはさまざまなヒューリスティクス（経験的優先規則）が用いられている

#### 最小コスト法 （最長一致法（深さ優先），文節数最小法，形態素数最小法など）を用いて素早く解析する。
- 最長一致法：ポインターから最も長い形態素を選択する
- 文節数最小法：文節数が最小になるように選択する
- 形態素数最小法：形態素数が最小になるように選択する
- 採用コスト法のアルゴリズムとしてはビタビアルゴリズムが用いられる
- 解析した形態素列 $M_0,…,M_{n+1}$ に対して，$\sum_{i=1}^{n} \text{形態素コスト}~M_i + \sum_{i=0}^{n} \text{連接コスト}~(M_i,M_{i+1})$ を最小化する

#### トライ：形態素辞書を木構造として，うまく検索しやすいようにしたもの
- ありえる形態素全て（動詞の未然形とかも含めた全ての形態素）を 1 文字目から順に木構造としている

### 統計モデルによる形態素解析
：自動的にコスト（パラメータ）を学習することによって，最小コスト法を実現する

#### 例：品詞 2 つ組モデル
目標：入力の文字列： $S = c_1,…,c_m$ $\rightarrow$ 形態素解析により，形態素列 $W_S=w_1,…,w_n$ と品詞列 $T_S = t_1,…,t_n$ を求める
- この時， $P(W_S,T_S | S) \approx \Pi_{i=1}^{n+1}P(t_i|t_{i-1})P(w_i|t_i)$ を最大化する組 $\hat{W_S},\hat{T_S}$ を求めれば良い
- 近似として，$P(t_i|t_{i-1}) = \frac{count(t_{i-1},t_i)}{count(t_{i-1})}$：品詞の遷移確率と，
$P(w_i|t_i) =\frac{count(w_i,t_i)}{count(t_i)}$：単語の出現確率を用いる

### 仮名漢字変換
：日本語特有で，入力のひらがな列中の任意の部分ひらがな列を，最小コスト法により一意の仮名漢字列に変換する
- ひらがな列： $K = k_1,…,k_m$
- 仮名漢字列： $W_K = w_1,…,w_m$
- $P(W_K | K) = \frac{P(K|W_K)P(W_K)}{P(K)}$ を最大化する
    - 言語モデル $P(W_K)=\Pi_{i=1}^{n+1} P(t_i|t_{i-1})P(w_i|t_i)$：$P(W_K)$ は単語列自体の確率分布
    - 仮名漢字モデル $P(K|W_K) = \Pi_{i=1}^{n+1} P(r_i|w_i)$：単語列 $W_K$ が与えられた時の $K$ の条件付き確率分布
        - 仮名漢字 $w_i$ に対してふりがな $r_i$ が付与される確率の積

---
## 構文解析
:::warning[目標]
文の構文構造に関する文法知識を用いて，品詞が同定された文の構文構造を同定する。
:::

### 句構造解析
：主に英文などで用いられる。文脈自由文法（CFG）を用いて，文を句構造の形式に変換する。

:::tip[文脈自由文法(CFG)]
形式文法 $G = (V_N,V_T,P,S)$ の4組で表される。木構造の形に書ける。
- $V_T$：終端記号（文の最後）の集合：**単語**
- $V_N$：非終端記号の集合：**品詞**
- $P$：文を生成する規則の集合（例： $S\rightarrow NP~VP,N\rightarrow girl$ など）：**文法**
    - 正確には，$ A \rightarrow \alpha~(A \in V_N, \alpha \in (V_N \cup V_T)^*)$
- $S$：開始記号
:::
- 句構造文法 > 文脈依存文法（CDG） > 文脈自由文法（CFG） > 正規文法（RG）
- 自然言語の多くは文脈自由文法で表現できる
- 同じ文字列でも異なる構文構造を持つことがある

#### 縦型句構造解析アルゴリズム：最もナイーブなアルゴリズム。2 通りの方法がある。
1. 葉（各単語 = 終端記号列）から組み上げる上昇型。すなわち，生成規則を逆にたどる形で，終端記号から非終端記号を生成し，開始記号に到達できれば成功

2. 開始記号 $S$ が与えられ，生成規則に従って句構造を成長させて，文章を構成できるまで頑張る下降型

#### CYK 法：文脈自由文法を用いた解析の代表的アルゴリズム。上昇型。$O(n^3)$。
生成規則をチョムスキー標準形 $A \rightarrow BC~\text{or}~A \rightarrow a~(A,B,C\in V_N,a\in V_T)$ に限定することで実現している。  
なお，任意の文脈自由文法は等価なチョムスキー標準形の文脈自由文法に変形できる。

1. 単語数 $n$ として，$n \times n$ の行列を用意し，対角成分を文章の終端記号( = 単語)と非終端記号( = 品詞)を埋める
2. $A \rightarrow BC$ を用いて，次々に非終端記号を埋めていく。非終端記号が 1 つの要素に複数入る場合もある
3. 1 行 n 列目に開始記号があれば成功。なければ失敗

#### 統計的手法：確率文脈自由文法に基づくモデルなど
- コーパスによって各生成規則に対してその確率値（= 頻度）が与えられており，コスト的なものも計算できる

### 係り受け解析
：日本語などで用いられる。比較的語順の制約が緩い，格要素が省略されやすい文の解析に使う
- 句構造解析では，全く同じ意味で異なる語順の文を同じ文として扱うことが難しい

**文節まとめ上げ規則** と **文節係り受け規則** からなる。二つの規則を区別することで語順の違いを考慮できる。
- 文節まとめ上げ規則：句構造で用いる文脈自由文法のように木構造を作るイメージ
- 文節係り受け規則：いわゆる修飾関係。単語の下に一つの単語がぶら下がっているイメージ。
    - 統計的手法により，文節 $b_i$ が文節 $b_j$ にかかる確率 $P(D) = \Pi P(b_i \rightarrow b_j)$ によって係り受け構造 $D$ を定義することもできる

---
## 意味解析
:::warning[目標]
用意しておいた意味的知識から，文の意味的整合性を判定し，文に対して整合性の取れた意味的解釈を与える。
:::
- 例えば多義語をどうするか

**意味的知識**：辞書として蓄積されているのが一般的だが，他に，意味素，シソーラス，格フレームなどでも表現される

### 意味素
：名詞を対象として意味の基本的分類を体系化して個々の名詞に分類を与えたもの。
- 例：「犬」は「動物」に分類される，さらに「動物」は「具体名詞」に分類される

### シソーラス
：意味や概念の体系を木構造で表現したもの。意味素より詳細な意味分類体系の場合が多い。
- 分類シソーラス：意味や概念の体系を作って，それに従って単語をグループ化する
- 上位・下位シソーラス：言葉を意味や概念によって階層的に整理する。木の節点全てに語が配置されている
- Roget, WordNet など

### 格フレーム
：動詞の用法ごとに格要素となりうる名詞の意味的制約を記述したもの。動詞ごとに，10,000 種類以上の格フレームがある。
- 食べる：「ガ格」は主体，「ヲ格」は食べ物，「ニ格」は場所，など。他の格フレームもあるかもしれない
- 人が頑張って作っている。より詳細に，「原因」などの意味役割を付与しようという動きもある
- 格フレームによる意味的制約：**選択制限**

#### 格：動詞に対して，文中の各語が持つ役割を表す
- 表層格：動詞に対する構文的な関係（主語，目的語：主格，目的格。格助詞で決定：ガ格……）
- 深層格：動詞に対して文中の各語がもつ意味的な関係（動作主格：動作を引き起こす対象……）

#### 格解析：格フレームとの整合性に基づいて意味解析を行う方法

### 語義曖昧性解消
：多義語の語義を決定する。多義語の定義文や例文との "類似度" を計算する方法や，機械学習で前後の文章との関係を学習する方法などがある。

### 語彙知識の獲得
：コーパスから言語知識を獲得する技術。昔は人文系の研究者が手作業でやっていた。
:::tip[共起]
語彙知識獲得のために，単語の共起関係を利用することが多い。

**共起：コーパス中の単語が同時に出現することであり，その確率が重要。相互情報量  $I(x,y) = \log_2 \frac{P(x,y)}{P(x)P(y)}$ が用いられることが多い。**
:::

#### 相互情報量：$I(x,y) = \log_2 \frac{P(x,y)}{P(x)P(y)} = \log_2 \frac{P(x|y)}{P(x)} = \log_2 \frac{P(y|x)}{P(y)}$
- $y$ が起きた時の方が $x$ が起きやすい/起きにくい $\rightarrow$ 相互情報量の絶対値が大きくなる
- この値自体は負を取りうるが，これを全部の確率について和をとったものは必ず 0 以上になる（これを相互情報量ということもある）
- 例えば，$P(x) = \frac{f(x)}{N},~P(y) = \frac{f(y)}{N},~P(x,y) = \frac{f(x,y)}{N}$ とする（ $f(*)$ はコーパス中での頻度）
- $I(x,y) \gg 0$：「けむり」と「くるま」，$I(x,y) \approx 0$：「けむり」と「きゅう」，$I(x,y) \ll 0$：「良い」と「悪い」

#### 名詞間の類似度の測定：例えば，同一の動詞と共起しやすい名詞の類似度は高いとして考える。
- ある動詞について，相互情報量の正負が同じの場合に，動詞と名詞の相互情報量の絶対値のうち小さい方を類似度とする。正負が異なる場合は類似度 0。
そして，全ての動詞に対して類似度を測り，その和を名詞間の類似度とする

#### 用言間の類似度の定義：例えば，同一の名詞と共起しやすい表現の類似度は高いとして考える。
- ”X solves Y” といった形を $p_i = X_i~expression_i~Y_i$ とする。
名詞 $n \in X_i$ と $p_i \in expression_i$ の相互情報量 $I_{X_i}(p_i,n)$ から，表現 $p_i,p_j$ の関係性を求めたい
- 例えば，$sim(X_i,X_j) = \frac{\sum_n (I_{X_i}(p_i,n) + I_{X_j}(p_j,n))}{\sum_n I_{X_i}(p_i,n) + \sum_n I_{X_j}(p_j,n)}$として
 $SIM(p_i,p_j) = \sqrt{sim(X_i,X_j)sim(Y_i,Y_j)}$ と定義することができる

---
## 文脈解析
:::warning[目標]
文同士の関係性を解析する。こそあど言葉や文同士のつながりなど。
:::

### 照応解析
:::tip[照応関係]
**照応関係**：省略やこそあど言葉を対象にして，ある言語表現が文脈内外の他の言語表現と同じ内容であること
- 照応詞：代名詞，省略（ = ゼロ代名詞）など，先行詞：照応詞に対応する言語表現（具体的な名詞など）。先行詞と照応詞は照応関係にある
:::
- 文脈照応：先行詞が文脈内にある存在する（ = 具体的な名詞など）場合
    - 前方照応：先行詞が照応詞より前にある：普通
    - 後方照応：先行詞が照応詞より後ろにある：ちょっと変
- 外界照応：先行詞が文脈外にある（ = 具体的でない）場合

### 中心化理論
：文脈照応を対象とする照応解析のモデルの一つ。文脈における局所的な話題や注意点（中心）の移り変わりをモデル化する。
:::tip[中心化理論のモデル]
#### 前向き中心 $Cf(U_i)$：たぶん今後の話題
文 $U_i$ において，その要素を次の文 $U_{i+1}$ で中心になる可能性の高い単語をリストで並べているもの
- 順序：文法主題・ゼロ主題 > 視点 > ガ格 > ニ格 > ヲ格 > その他
- 最も序列が高い( = 中心になる可能性が高いもの)：優先中心 $Cp(U_i)$ 

#### 後ろ向き中心 $Cb(U_i)$：今の話題
文 $U_i$ において，その内容の中心となっている特別な要素
:::

#### 中心化理論の制約
1. ただ一つの後ろ向き中心 $Cb(U_i)$ が存在する
2. 前向き中心 $Cf(U_i)$ のどの要素も $U_i$ の中に存在する
3. 後ろ向き中心 $Cb(U_i)$ は $Cf(U_{i-1})$ の要素のうち $U_i$ に出現し，かつ $Cf(U_{i-1})$ の中で最も序列が高かったもの
    - $Cb(U_i) = Cp(U_{i-1})~(\text{if}~Cp(U_{i-1}) \in U_i)$

#### 中心化理論の規則
1. $Cf(U_{i-1})$ のある要素が $U_i$ に代名詞として出現するなら，$Cf(U_i)$ にも代名詞として出現する
2. $Cb$ の発話間の遷移は 4 種類ある。上から優先順位が高い
    1. Continue（後ろ向き中心前と変わらず，後ろ向き中心 = 優先中心）
    2. Retain（後ろ向き中心前と変わらず，後ろ向き中心 ≠ 優先中心）
    3. smooth shift（後ろ向き中心前と変わり，後ろ向き中心 = 優先中心）
    4. rough shift（後ろ向き中心前と変わり，後ろ向き中心 ≠ 優先中心）

### 修辞構造解析（RST）
：文や節などの文章中の論理的な単位を認定し，その関係性（ = 修辞関係 ）を定義する。イメージは接続詞。
- 核：修飾される中心の部分，衛星：核を修飾する部分。例：核 = 「結果」，衛星 = 「原因」