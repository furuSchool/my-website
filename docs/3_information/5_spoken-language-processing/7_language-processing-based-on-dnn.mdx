---
title: 7. 深層学習に基づく言語処理
---

## 単語の分散表現
：単語をベクトル化する。embedding とも呼ばれる

### 例
skip-gram では，単語列 $w_1,…,w_T$，単語ベクトル $w_t~(t= 1,…,T)$として前後各 $c$ 単語のベクトルを予測するための
目的関数を最大化して単語ベクトルを学習する
- 目的関数：$\frac{1}{T}\sum_i\sum_{-c\le j\le c} \log P(w_{t+j}|w_t)$
- 出力の確率分布は softmax関数を用いる：$P(w_{t+j}|w_t) = \frac{\exp((v'_{w_{t+j}})^T)v_{w_t}}{\sum_{w=1}^W \exp((v'_{w})^T)v_{w_t}}$
- 任意の単語の one-hot ベクトル $\bm{w}$ は $v_w = E\bm{w},~v'_w = Z\bm{w}$ によって表されるので，
$E, Z$ （プロジェクション行列。全ての単語ベクトルの情報を持つ）を求めればよい
    - $v_w$：入力層の単語ベクトル，$v'_w$：出力層の単語ベクトル

別の例では，CBOW では入力を $w_t$ の前後数単語を入れて $w_t$ を予測して単語ベクトルを学習する

## 依存構造解析
：構文解析（品詞が同定された文の構文構造を同定するタスク）の一つの方法。単語同士の依存関係を解析する。文章の依存構造を求める際に，ニューラルネットワークを用いる。

## 文の分類
：例えば，文章の意見の肯定/否定を分類するタスク。CNN などが用いられる。
1. 入力：単語数（ $m$ 語） $\times$ 単語ベクトル（ $d$ 次元）の $m \times d$ の行列（文章が行，それぞれの単語ベクトルが列）
2. $h \times d$ のフィルタ行列を $k$ 種類用意しフィルタで畳み込み，ベクトル（ $m-h+1$ 次元）を得る
3. それぞれのベクトルを，最大プーリングして最も大きい要素を抽出。合体して $k$ 次元のベクトルを得る
4. softmax 関数・正規化で二値クラス判定を行う