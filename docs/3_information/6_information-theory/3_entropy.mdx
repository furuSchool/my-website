---
title: 3. 情報量
---

## 情報量
:::tip[情報量]
情報源符号化の際の 1 情報源記号あたりの平均符号長の下限を **情報量 (単位はビット)** と定義する。　　

したがって，情報源記号 $a_1,…,a_M$ を確率 $p_1,…,p_M$ で発生する無記憶記憶定常情報源は，  
1 情報源あたり $H(S) = - \sum_{i=1}^M p_i \log_2 p_i$ の情報量を持つ。
- この定義による情報量を平均情報量と呼ぶこともある
:::
なお，直感的には，確率 $p$ の結果の生起が分かった時に得られる情報量 $I(p)$ とすると，以下の条件を満たして欲しい。
1. $I(p)$ は $p$ についての単調減少関数 ($p$ が小さいことが起こったほど得られる情報量は大きい)
2. $I(p_1) + I(p_2) = I(p_1 p_2)$ ($p_1$ と $p_2$ が独立に起こる時，情報量は単に足し合わせたものになってほしい)
3. $I(p)$ が $p$ の連続関数である

実は，これらを満たすのは $I(p) = - a\log_2 p$ という形でしかあり得ない。

---
## エントロピー
:::tip[エントロピー]
情報源 $S$ のエントロピー $H(S)$ は，$S$ のある時点の出力を知る以前に持っている「知識のあいまいさ」の指標である。  
つまり，「情報量 = その情報を受け取ることによるエントロピーの減少量」と定義される。

例えば，情報源アルファベット $\{a_1,…,a_M\}$ の定常分布が $P(a_i) = p_i$ で与えられる無記憶定常情報源 $S$ のエントロピーは，
$H(S) = - \sum_{i=1}^M p_i \log_2 p_i$ である。
:::
- 情報源アルファベットが $M$ 個のエントロピーは $0 \leq H(S) \leq \log_2 M$ を満たす。等号成立は，情報源アルファベットが等確率で発生する場合
- また，冗長度 $\rho(S) = 1 - \frac{H(S)}{\log_2 M}$ を定義する

---
## 相互情報量
### 条件付きエントロピー
:::tip[条件付きエントロピー]
情報源 $S$ の条件付きエントロピー $H(S|T)$ は，情報源 $T$ の出力を知った後の情報源 $S$ のエントロピーである。

$P(x),P(y)$ をそれぞれ情報源 $S$ と $T$ の確率分布，$P(x,y)$ を情報源 $S$ と $T$ の同時確率分布，$P(x|y) = \frac{P(x,y)}{P(y)}$ を条件付き確率分布とする。

この時，$H(S|T) = - \sum_y P(y) \sum_x P(x|y) \log_2 P(x|y) = - \sum_{x,y} P(x,y) \log_2 P(x|y)$ となる。
:::
- 確率分布とは，確率変数に対して，各々の値を取る確率全体を表すもの
- この場合の確率変数は情報源 $S$ と $T$ であり，それぞれ確率分布に従って出力を行う

### 相互情報量
:::tip[相互情報量]
二つの情報源 $X,Y$ が互いに関わっている時，$X$ を知ることで減少した $Y$ のエントロピーの量を **相互情報量** $I(X;Y)$ と呼ぶ。

相互情報量は，
- $I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) = H(X) + H(Y) - H(X,Y)$
    - **結合エントロピー** $H(X,Y) = - \sum_{x,y} P(x,y) \log_2 P(x,y)$ ($X,Y$ をまとめて考えた時のエントロピー)
- $I(X;Y) = \sum_{x,y} P(x,y) \log_2 \frac{P(x,y)}{P(x)P(y)}$
:::
- 相互情報量は二つの確率変数間の「共通部分」を表す量である
- $I(X;Y) = I(Y;X)$ である
- $I(X;Y) = 0$ の時は $X$ と $Y$ が独立である
- $0 \leq I(X;Y) \leq H(X), H(Y)$ である

### 相対エントロピー
:::tip[相対エントロピー]
二つの情報源 $X,Y$ の **相対エントロピー** $D(X||Y)$ は，$D(X||Y) = \sum_x P_X(x) \log_2 \frac{P_X(x)}{P_Y(x)}$ で表される。
- $P_X(x)$ は情報源 $X$ の確率分布，$P_Y(x)$ は情報源 $Y$ の確率分布
:::
- 相対エントロピー $D(X||Y)$ は，確率変数間の「差」を表す量である
- 一般には $D(X||Y) \neq D(Y||X)$ である
- $I(X;Y) = D(P_{X, Y}(x,y)||P_X(x)P_Y(y))$ である

画像は例で[こちら](https://whyitsso.net/math/statistics/information_entropy.html)より引用
<img src="/img/information/entropy.png" alt="エントロピーの可視化" style={{ width: '50%' }} />

---
## ひずみを考慮したエントロピー
:::tip[ひずみ測度]
情報源 $S$ の出力を情報源符号化し，復号した時の結果を $y$ とする。この時，その相違を測るための指標を **ひずみ測度** $d(x,y)$ と呼ぶ。

また，**平均ひずみ** $\overline{d}(S) = \sum_{x,y} P(x,y) d(x,y)$ を定義する。
:::
- 例 : 情報源アルファベット $\{0,1\}$，$d(x,y) = \begin{cases} 0 & (x = y) \\ 1 & (x \neq y) \end{cases}$ の場合，
$\overline{d}(S) = P(0,1) + P(1,0)$ **(ビット誤り率と呼ばれる)**
- 例 : 二乗平均誤差 $d(x,y) = (x-y)^2$

:::tip[ひずみが許される場合の情報源符号化定理]
平均ひずみ $\overline{d}(S)$ を $D$ 以下に抑える条件下で，
情報源 $S$ を 1 情報源記号当たりの平均符号長 $L$ が  
$\min_{\overline{d}(S) \leq D} {I(X; Y)} \leq L < \min_{\overline{d}(S) \leq D} {I(X; Y)} + \epsilon$ を満たす瞬時符号を作成できる。
- ただし，$R(D) = \min_{\overline{d}(S) \leq D} {I(X; Y)}$ は **速度-ひずみ関数** と呼ばれる
:::
- $I(X;Y)$ が同じ情報源符号化の方法でも，平均ひずみ $\overline{d}(S)$ は一般には異なる