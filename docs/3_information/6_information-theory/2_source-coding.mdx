---
title: 2. 情報源符号化
---

:::warning[目標]
情報源符号化の方法とその限界を理解する。すなわち，与えられた情報源記号に対して，どのような情報源符号化を施して新たな符号語とすれば良いのかを考える！  
ただし以下では，特記がない限り **2元符号** を扱うことにする。
:::

---
## 情報源符号化の基礎
### 用語の定義
- 特異符号：異なる情報源記号に同じ符号語が割り当てられる。もちろんあり得ない
- 一意復号可能：復号が可逆であること。もちろん一意復号可能でないと困る
    - 一意復号不可能な例：(A, B, C, D) を (0, 10, 11, 01) と割り当てると，「0110」は「DB」か「ACA」かわからない
- **瞬時符号**：符号語を前から順に見た時，ただちに復号できる符号。望ましい
    - 非瞬時符号の例：(A, B, C, D) を (0, 01, 011, 111) と割り当てると，「0111…」は，次の符号によって「AD」か「BD」か「CD」わからない
- 等長符号：符号語の長さが全て同じ。
- **コンパクト符号**：情報源記号を一つずつ一意復号可能な符号に符号化する時，平均符号長が最小となる符号

:::tip[情報源符号化の条件]
1. 一意復号可能であること。瞬時符号であることが望ましい
2. 1 情報源記号あたりの平均符号長ができるだけ短い
3. その他。装置化があまり複雑にならない，使用周波数帯域が少ない，など
:::

### 瞬時符号の条件
<img src="/img/information/prefix-code.png" alt="瞬時符号" style={{ width: '45%' }} />
画像は [こちら](https://www.mnc.toho-u.ac.jp/v-lab/yobology/coding_and_entropy/coding_and_entropy.htm) より引用

**瞬時符号であるためには，どの符号も他の符号語の語頭にはなってはならない。（語頭条件）**  
すなわち，図のように符号の木を書くと，情報源記号に割り当てられる符号語はすべて木における葉に対応する。

### クラフトの不等式
:::tip[クラフトの不等式]
長さ $l_1,…,l_M$ となる $M$ 個の符号語を持つ $q$ 元符号が瞬時符号となるための必要十分条件は，以下の不等式を満たすことである。
$$
q^{-l_1} + \cdots + q^{-l_M} \le 1
$$
:::
- 直感的には，符号の木を考えれば簡単にわかる
- ちなみに，この不等式は一意復号可能な符号が存在するための必要十分条件でもある（マクミランの不等式）

### 平均符号長の限界
情報源 $S$ について，情報源アルファベット $\{a_1,…,a_M\}$ の定常分布が $P(a_i) = p_i$ で与えられるとする。  
$a_i$ に符号語 $l_i$ を割り当てるとすると，**平均符号長** $L = l_1p_1 + \cdots + l_Mp_M$ と表される。

:::note[数学的な補助定理(シャノンの補助定理)]
$p_1 + \cdots p_M = 1$ かつ $p_1,…,p_M \in \mathbb{R_{\ge 0}}$ とする。（定常分布の確率分布のイメージ）  
この時，$q_1 + \cdots q_M \le 1$ を満たす任意の $q_1,…,q_M \in \mathbb{R_{\ge 0}}$ について，以下の不等式が成り立つ。
$$
- \sum_{i = 1}^M p_i \log_2 q_i \ge - \sum_{i = 1}^M p_i \log_2 p_i = H_1(S)
$$
等号成立は，$q_i = p_i~(i = 1, …, M)$ の時。
:::

そして，この定理を用いると，以下のことが言える。
:::tip[定理]
先の情報源 $S$ の各情報源記号を一意復号可能な瞬時符号に符号化した時，以下の二つを満たす。
1. 必ず，平均符号長 $L \ge - \sum_{i = 1}^M p_i \log_2 p_i = H_1(S)$ となる
2. 平均符号長が $L \le H_1(S) + 1$ なる瞬時符号を作成できる

ここで，$H_1(S) = - \sum_{i = 1}^M p_i \log_2 p_i$ を **情報源 $S$ の一次エントロピー** と呼ぶ
:::

- $q$ 元符号の場合は，$\displaystyle H_1(S) = - \sum_{i = 1}^M p_i \log_q p_i$ となる
---
## ハフマン符号
:::tip[ハフマン符号]
ハフマン符号は，情報源記号を一つずつ一意復号可能な符号に符号化する時に，平均符号長が最小となる符号 (コンパクト符号) の一種。
:::
### ハフマン符号の作り方
1. 各情報源記号に対応する葉を作る
2. 確率が最も小さい2枚の葉・節点に対し，節点を作成して結び，それぞれ 0 と 1 を割り当てる
3. 上記を繰り返す

- 同じ確率の葉がある場合はどの葉を選んでも良い $\to$ ハフマン符号が複数存在する可能性がある
- これがコンパクト符号であることは簡単に証明できる
- $q$ 元符号である場合は，情報源記号の数をダミーを入れて $(q-1)m + 1~(m \in \mathbb{Z})$ にしてからハフマン符号化すれば良い

<img src="/img/information/huffman-coding.png" alt="ハフマン符号" style={{ width: '45%' }} />
画像は [こちら](http://www.snap-tck.com/room03/c02/comp/comp032.html) より引用。(a,b,c,d) が (0,10,110,111) に割り当てられている。

---
## 情報源符号化定理
何個かの情報源記号をまとめて符号化することを考える。

**ブロック符号化**：一定個数の情報源記号をまとめて符号化すること $\to$ できた符号は **ブロック符号** と呼ばれる。
- 例：情報源アルファベットを $\{a,b\}$ とし，$2$ 個まとめた $\{aa, ab, ba, bb\}$ を符号化する

このようにして，$M$ 元情報源 $S$ の $n$ 個の情報源記号をまとめて符号化する時，それ自体を一つの情報源とみなして **$n$ 次の拡大情報源 $S^n$** と呼ぶ。
- 例：情報源アルファベットが $\{a_1,…,a_M\}$ である $M$ 元情報源 $S$ $\to$ $n$ 次の拡大情報源 $S^n$ の情報源アルファベットは $\{a_1,…,a_M\}^n$ となる

ここで定理より，$\displaystyle H_1(S^n) \leq L_n \le H_1(S^n) + 1$ を満たす瞬時符号を作成できる。($L_n$ は $S^n$ の平均符号長)

そこで，**$S$ の $n$ 次エントロピー $\displaystyle H_n(S) = \frac{H_1(S^n)}{n}$** と **$S$ のエントロピー $H(S) = \lim_{n \to \infty} H_n(S)$** を定義する。この時，
- $\displaystyle H_n(S) \leq L \le H_n(S) + \frac{1}{n}$ ($L$ は $S$ の平均符号長) と変形できる
- $H(S) \leq H_n(S)$ が成り立つ

この二つより，**情報源符号化定理**が成り立つ。
:::tip[情報源符号化定理]
情報源 $S$ は，任意の正の数 $\epsilon$ に対し，以下のような条件を満たす瞬時符号を作成できる。
$$
H(S) \leq L \le H(S) + \epsilon
$$
一方で，$L < H(S)$ となる瞬時符号は存在しない
- **$S$ の $1$ 次エントロピー：$\displaystyle H_1(S) = - \sum_{i=1}^M p_i \log_2 p_i$**
- **$S^n$ の $1$ 次エントロピー：$\displaystyle H_1(S^n) = - \sum_{x_0, \cdots, x_{n-1}} P(x_0, x_1, \cdots, x_{n-1}) \log_2 P(x_0, x_1, \cdots, x_{n-1})$**
    - $2^n$ 元符号のイメージ。
- **$S$ の $n$ 次エントロピー：$\displaystyle H_n(S) = \frac{H_1(S^n)}{n}$**
- **$S$ のエントロピー：$H(S) = \lim_{n \to \infty} H_n(S)$**
:::
- $q$ 元符号の場合は，$H_1(S) = - \sum_{i=1}^M p_i \log_q p_i$ ならそのまま成り立ち，
$H_1(S) = - \sum_{i=1}^M p_i \log_2 p_i$ の場合は，$\frac{H(S)}{\log_2 q} \leq L \le \frac{H(S)}{\log_2 q} + \epsilon$ となる

---
## 基本的な情報源のエントロピー
### 無記憶定常情報源
情報源 $S$ について，情報源アルファベット $\{a_1,…,a_M\}$ の定常分布が $P(a_i) = p_i$ で与えられるとする。
また，無記憶なので，$P(x_0, x_1, \cdots, x_{n-1}) = \Pi_{i=0}^{n-1} P(x_i)$ である。

この時，$\displaystyle H_1(S^n) = n H_1(S)$ となる。
- 証明：$H_1(S^n) = - \sum_{x_0, \cdots, x_{n-1}} P(x_0) \cdots P(x_{n-1}) \log_2 P(x_0) \cdots P(x_{n-1}) = \\- \sum_{x_0} P(x_0) \log_2 P(x_0) - \cdots - \sum_{x_{n-1}} P(x_{n-1}) \log_2 P(x_{n-1}) = n H_1(S)$

ここで，エントロピー関数 $\mathcal{H}(x) = -x \log_2 x - (1-x) \log_2 (1-x)$ と定義する。
この時，**2元無記憶定常情報源のエントロピー**は，$\displaystyle H(S) = \mathcal{H}(p)$ となる。($p$, $1-p$ はそれぞれ情報源記号の発生確率)