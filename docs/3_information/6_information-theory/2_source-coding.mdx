---
title: 2. 情報源符号化
---

:::warning[目標]
情報源符号化の方法とその限界を理解する。すなわち，与えられた情報源記号に対して，どのような情報源符号化を施して新たな符号語とすれば良いのかを考える！  
ただし以下では，特記がない限り **2元符号** を扱うことにする。
:::

---
## 情報源符号化の基礎
### 用語の定義
- 特異符号：異なる情報源記号に同じ符号語が割り当てられる。もちろんあり得ない
- 一意復号可能：復号が可逆であること。もちろん一意復号可能でないと困る
    - 一意復号不可能な例：(A, B, C, D) を (0, 10, 11, 01) と割り当てると，「0110」は「DB」か「ACA」かわからない
- **瞬時符号**：符号語を前から順に見た時，ただちに復号できる符号。望ましい
    - 非瞬時符号の例：(A, B, C, D) を (0, 01, 011, 111) と割り当てると，「0111…」は，次の符号によって「AD」か「BD」か「CD」わからない
- 等長符号：符号語の長さが全て同じ。
- **コンパクト符号**：情報源記号を一つずつ一意復号可能な符号に符号化する時，平均符号長が最小となる符号

:::tip[情報源符号化の条件]
1. 一意復号可能であること。瞬時符号であることが望ましい
2. 1 情報源記号あたりの平均符号長ができるだけ短い
3. その他。装置化があまり複雑にならない，使用周波数帯域が少ない，など
:::

### 瞬時符号の条件
<img src="/img/information/prefix-code.png" alt="瞬時符号" style={{ width: '45%' }} />
画像は [こちら](https://www.mnc.toho-u.ac.jp/v-lab/yobology/coding_and_entropy/coding_and_entropy.htm) より引用

**瞬時符号であるためには，どの符号も他の符号語の語頭にはなってはならない。（語頭条件）**  
すなわち，図のように符号の木を書くと，情報源記号に割り当てられる符号語はすべて木における葉に対応する。

### クラフトの不等式
:::tip[クラフトの不等式]
長さ $l_1,…,l_M$ となる $M$ 個の符号語を持つ $q$ 元符号が瞬時符号となるための必要十分条件は，以下の不等式を満たすことである。
$$
q^{-l_1} + \cdots + q^{-l_M} \le 1
$$
:::
- 直感的には，符号の木を考えれば簡単にわかる
- ちなみに，この不等式は一意復号可能な符号が存在するための必要十分条件でもある（マクミランの不等式）

### 平均符号長の限界
情報源 $S$ について，情報源アルファベット $\{a_1,…,a_M\}$ の定常分布が $P(a_i) = p_i$ で与えられるとする。  
$a_i$ に符号語 $l_i$ を割り当てるとすると，**平均符号長** $L = l_1p_1 + \cdots + l_Mp_M$ と表される。

:::note[数学的な補助定理(シャノンの補助定理)]
$p_1 + \cdots p_M = 1$ かつ $p_1,…,p_M \in \mathbb{R_{\ge 0}}$ とする。（定常分布の確率分布のイメージ）  
この時，$q_1 + \cdots q_M \le 1$ を満たす任意の $q_1,…,q_M \in \mathbb{R_{\ge 0}}$ について，以下の不等式が成り立つ。
$$
- \sum_{i = 1}^M p_i \log_2 q_i \ge - \sum_{i = 1}^M p_i \log_2 p_i = H_1(S)
$$
等号成立は，$q_i = p_i~(i = 1, …, M)$ の時。
:::

そして，この定理を用いると，以下のことが言える。
:::tip[定理]
先の情報源 $S$ の各情報源記号を一意復号可能な瞬時符号に符号化した時，以下の二つを満たす。
1. 必ず，平均符号長 $L \ge - \sum_{i = 1}^M p_i \log_2 p_i = H_1(S)$ となる
2. 平均符号長が $L \le H_1(S) + 1$ なる瞬時符号を作成できる

ここで，$H_1(S) = - \sum_{i = 1}^M p_i \log_2 p_i$ を **情報源 $S$ の一次エントロピー** と呼ぶ
:::

- $q$ 元符号の場合は，$\displaystyle H_1(S) = - \sum_{i = 1}^M p_i \log_q p_i$ となる
---
## ハフマン符号
:::tip[ハフマン符号]
ハフマン符号は，情報源記号を一つずつ一意復号可能な符号に符号化する時に，平均符号長が最小となる符号 (コンパクト符号) の一種。
:::
### ハフマン符号の作り方
1. 各情報源記号に対応する葉を作る
2. 確率が最も小さい2枚の葉・節点に対し，節点を作成して結び，それぞれ 0 と 1 を割り当てる
3. 上記を繰り返す

- 同じ確率の葉がある場合はどの葉を選んでも良い $\to$ ハフマン符号が複数存在する可能性がある
- これがコンパクト符号であることは簡単に証明できる
- $q$ 元符号である場合は，情報源記号の数をダミーを入れて $(q-1)m + 1~(m \in \mathbb{Z})$ にしてからハフマン符号化すれば良い

<img src="/img/information/huffman-coding.png" alt="ハフマン符号" style={{ width: '45%' }} />
画像は [こちら](http://www.snap-tck.com/room03/c02/comp/comp032.html) より引用。(a,b,c,d) が (0,10,110,111) に割り当てられている。

---
## 情報源符号化定理
何個かの情報源記号をまとめて符号化することを考える。

**ブロック符号化**：一定個数の情報源記号をまとめて符号化すること $\to$ できた符号は **ブロック符号** と呼ばれる。
- 例：情報源アルファベットを $\{a,b\}$ とし，$2$ 個まとめた $\{aa, ab, ba, bb\}$ を符号化する

このようにして，$M$ 元情報源 $S$ の $n$ 個の情報源記号をまとめて符号化する時，それ自体を一つの情報源とみなして **$n$ 次の拡大情報源 $S^n$** と呼ぶ。
- 例：情報源アルファベットが $\{a_1,…,a_M\}$ である $M$ 元情報源 $S$ $\to$ $n$ 次の拡大情報源 $S^n$ の情報源アルファベットは $\{a_1,…,a_M\}^n$ となる

ここで定理より，$\displaystyle H_1(S^n) \leq L_n \le H_1(S^n) + 1$ を満たす瞬時符号を作成できる。($L_n$ は $S^n$ の平均符号長)

そこで，**$S$ の $n$ 次エントロピー $\displaystyle H_n(S) = \frac{H_1(S^n)}{n}$** と **$S$ のエントロピー $H(S) = \lim_{n \to \infty} H_n(S)$** を定義する。この時，
- $\displaystyle H_n(S) \leq L \le H_n(S) + \frac{1}{n}$ ($L$ は $S$ の平均符号長) と変形できる
- $H(S) \leq H_n(S)$ が成り立つ

この二つより，**情報源符号化定理**が成り立つ。
:::tip[情報源符号化定理]
情報源 $S$ は，任意の正の数 $\epsilon$ に対し，以下のような条件を満たす瞬時符号を作成できる。
$$
H(S) \leq L \le H(S) + \epsilon
$$
一方で，$L < H(S)$ となる瞬時符号は存在しない
- **$S$ の $1$ 次エントロピー：$\displaystyle H_1(S) = - \sum_{i=1}^M p_i \log_2 p_i$**
- **$S^n$ の $1$ 次エントロピー：$\displaystyle H_1(S^n) = - \sum_{x_0, \cdots, x_{n-1}} P(x_0, x_1, \cdots, x_{n-1}) \log_2 P(x_0, x_1, \cdots, x_{n-1})$**
    - $2^n$ 元符号のイメージ。
- **$S$ の $n$ 次エントロピー：$\displaystyle H_n(S) = \frac{H_1(S^n)}{n}$**
- **$S$ のエントロピー：$H(S) = \lim_{n \to \infty} H_n(S)$**
:::
- $q$ 元符号の場合は，$H_1(S) = - \sum_{i=1}^M p_i \log_q p_i$ ならそのまま成り立ち，
$H_1(S) = - \sum_{i=1}^M p_i \log_2 p_i$ の場合は，$\frac{H(S)}{\log_2 q} \leq L \le \frac{H(S)}{\log_2 q} + \epsilon$ となる

---
## 基本的な情報源のエントロピー
### 無記憶定常情報源
情報源 $S$ について，情報源アルファベット $\{a_1,…,a_M\}$ の定常分布が $P(a_i) = p_i$ で与えられるとする。
また，無記憶なので，$P(x_0, x_1, \cdots, x_{n-1}) = \Pi_{i=0}^{n-1} P(x_i)$ である。

この時，$\displaystyle H_1(S^n) = n H_1(S)$ となる。
- 証明：$H_1(S^n) = - \sum_{x_0, \cdots, x_{n-1}} P(x_0) \cdots P(x_{n-1}) \log_2 P(x_0) \cdots P(x_{n-1}) = \\- \sum_{x_0} P(x_0) \log_2 P(x_0) - \cdots - \sum_{x_{n-1}} P(x_{n-1}) \log_2 P(x_{n-1}) = n H_1(S)$

従って，$\displaystyle H(S) = H_1(S) = - \sum_{i=1}^M p_i \log_2 p_i$ となる。
- エントロピー関数 $\mathcal{H}(x) = -x \log_2 x - (1-x) \log_2 (1-x)$ を定義する。
この時，**2元無記憶定常情報源のエントロピー**は，$\displaystyle H(S) = \mathcal{H}(p)$ となる。($p$, $1-p$ はそれぞれ情報源記号の発生確率)

### マルコフ情報源
情報源 $S$ について，情報源アルファベット $\{a_1,…,a_M\}$，状態 $s_0,…,s_{N-1}$，定常分布 $(\omega_0,…,\omega_{N-1})$，
状態 $s_i$ において情報源記号 $a_j$ を発する確率を $P(a_j|s_i)$ とする。 

この時，$\displaystyle H(S) = - \sum_{i=0}^{N-1} \omega_i \sum_{j=1}^M P(a_j|s_i) \log_2 P(a_j|s_i)$ となる。
- 状態 $s_i$ にいる時は確率分布 $P(a_j|s_i)$ の無記憶定常情報源と同じなので，エントロピーは $- \sum_{j=1}^M P(a_j|s_i) \log_2 P(a_j|s_i)$

---
## 基本的な情報源符号化法
### ハフマンブロック符号化
十分大きな $n$ に対して，$n$ 次情報源をハフマン符号化する。
理論的には可能だが，平均符号長は $\frac{1}{n}$ の速さでしか下限 $H(S)$ に近づかない一方，情報源系列数は $M^n$ ($M$ はアルファベットの元の数) で増大するので，実用的ではない。

### 非等長符号化
確率に応じて非等長の情報源のブロックを作成してからハフマン符号化などを行う方法。
- 例：1,0 を確率 $0.2,0.8$ で出力する無記憶情報源を考える。$1, 01, 001, 000$ の情報源系列に対してハフマン符号化を行う
- 情報源系列の平均長をなるべく大きくしたい！が，理論的に行うのは難しい
    - **情報源系列の平均長** は大きくしたく，**平均符号長 $L$** は小さくしたい。混乱しないように
    - 情報源系列の平均長は $H(S) \le L \le H(S) + \frac{1}{n}$ の $n$ に効いてくる

### ランレングス符号化
非等長符号化の一種。情報源系列の中で同じ情報源記号が連続する長さを符号化する。
- 例：0の方が多く出力される場合，長さ 3 まで符号化すれば，$0110000010001$ は $1, 0, 3, 2, 3, …$ となる

一般に，無記憶2元情報源について，1, 0 の出力確率が $p, 1-p~(p < 1-p)$ であり，長さ $N-1$ までランレングス符号化を行うとすると，
- 情報源系列の平均長： $\displaystyle \bar{n} = \frac{1 - (1-p)^{N-1}}{p}$
- 上記の情報源系列に対してハフマン符号化を行うと，平均符号長は $\displaystyle L < H(S) + \frac{1}{\bar{n}}$ となる
    - 普通に $n$ 次の拡大情報源でハフマン符号化を行うと，平均符号長は $\displaystyle L < H(S) + \frac{1}{n}$ となる

**従って，$p$ が小さいほどランレングス符号化 + ハフマン符号化の方が良い。**

### 算術符号化
長さ $n$ の情報源系列を符号化する方法の一つ。**コンパクト符号** とは限らない。

情報源アルファベット $\{a_1,…,a_M\}$ とし，そこから，長さ $n$ の情報源系列 $x_0, x_1, \cdots, x_{n-1}$ が得られたとする。  
**この時，累積確率 $C(x_0, x_1, \cdots, x_{n-1})$ を定義する。** すなわち，$x_0, x_1, \cdots, x_{n-1}$ より "前" にある情報源記号の確率を足し合わせる。
- 例：$C(a_1, \cdots, a_1) = P(a_1, \cdots, a_1),~ C(a_1, \cdots, a_1, a_2) = P(a_1, \cdots, a_1) + P(a_1, \cdots, a_1, a_2)$ のようになる

ここで，情報源系列 $x_0, x_1, \cdots, x_{n-1}$ を $\bm{a}_i~(i = 1, \cdots, M^n)$ とおくと，
$C(\bm{a}_i) = \sum_{j=1}^{i-1} P(a_j)$ となる。
この時，$C(\bm{a}_{i})$ が $C(\bm{a}_{i-1}),~C(\bm{a}_{i+1})$ と区別できる桁数まで数値を取って符号とする。
- 例：$C(\bm{a}_0) = 0, C(\bm{a}_1) = 0.343 = 0.0101…, C(\bm{a}_2) = 0.49 = 0.01111…$ の場合，
$\bm{a}_1$ の符号は $010$ である ($C(\bm{a}_0)$ より大きく $\bm{a}_0$ は $00$ である)
- この時，$C(\bm{a}_i)$ の小数点以下の桁数は $max[-\log_2 P(a_i), -\log_2 P(a_{i-1})]$ となる

そして，算術符号化では一般に，できるだけ長い $n$ をとって情報源系列を符号化するのが良い。
画像は例で[こちら](https://fussy.web.fc2.com/algo/compress10_arithmetic.htm)より引用 (この場合は $abacab$ を符号化している)
<img src="/img/information/arithmetic-coding.png" alt="算術符号化" style={{ width: '50%' }} />

なお，平均符号長は $\lim_{n \to \infty} L_n = H(S)$ となるが，$P(a_i)$ を無限の精度で求めることはできないので一定桁数に制限する。
つまり，$P(\bm{a}_i)$ を $\frac{1}{2^k}$ の形に近似する。
- 無記憶情報源の場合は，$P(x_i)$ を $\frac{1}{2^k}$ の形に近似すればよい
    - 問題なのは，$P(\bm{a}_i)$ を求める際に，$P(x_i)$ を乗算する必要があり，その際に誤差が大きくなること

### 算術符号化と復号の例
1, 0 を確率 $0.25, 0.75$ で出力する無記憶情報源を考え，$100101$ を符号化する。画像のイメージを忘れないように (というか，それさえわかれば大丈夫)
- $P(\lambda) = 1, C(\lambda) = 0$
- $P(0) = 0.11, P(1) = 0.01$
- $P(\bm{x}0) = < P(\bm{x}) - P(\bm{x}1) >_m, P(\bm{x}1) = P(\bm{x})P(1), m=2$（区別できれば良い）
- $C(\bm{x}0) = C(\bm{x}), C(\bm{x}1) = C(\bm{x}) + P(\bm{x}0)$

$\bm{x}$|$x$|$P(\bm{x})$|$P(\bm{x}0)$|$P(\bm{x}1)$|$C(\bm{x}x)$|
---|-----|---|---|---|---
$\lambda$|1|1|0.11|0.01|0.11
1|0|0.01|0.0011|0.0001|0.11
10|0|0.0011|0.0010|0.000011|0.11
100|1|0.0010|0.00011|0.000010|0.11011
1001|0|0.000010|0.0000011|0.00000010|0.11011
10010|1|0.0000011|0.0000010|0.000000011|0.1101110

また，符号化された $\bm{x} = 1101110$ を復号する。
- $\bm{x_0} = \lambda$
- $
\bm{x}_{k+1} =
\begin{cases}
\bm{x_k}0, & \text{if } C(\bm{x}) < C(\bm{x_k}) + P(\bm{x_k}0) \\
\bm{x_k}1, & \text{if } C(\bm{x}) \geq C(\bm{x_k}) + P(\bm{x_k}1)
\end{cases}
$

$k$|$\bm{x}_k$|$C(\bm{x}_k)$|$C(\bm{x})$|$C(\bm{x}_k) + P(\bm{x}_k0)$
---|---|---|---|---
0|$\lambda$|0|0.1101110|0.11
1|1|0.11|0.1101110|0.1111
2|10|0.11|0.1101110|0.1110
3|100|0.11011|0.1101110|0.11011
4|1001|0.11011|0.1101110|0.1101111
5|10010|0.1101110|0.1101110|0.1101110
6|100101|0.1101110|---|---

:::note[まとめ]
- ハフマンブロック符号化 : 十分大きな $n$ に対して，$n$ 次情報源をハフマン符号化する。実用的でない
- ランレングス符号化 : 情報源系列の中で同じ情報源記号が連続する長さを符号化する。特に情報源記号に偏りがある場合に有効
- 算術符号化 : 長さ $n$ の情報源系列によって，0 ~ 1 を分割して符号化するイメージ
:::

---
## ユニバーサル符号化
:::warning[目標]
これまでは，情報源の確率分布がわかっていることを前提にしていた。
ここでは，情報源の確率分布がわからない場合にも使える **ユニバーサル符号化** について考える。
:::

### 典型的系列
ここで，情報源記号 $a_1, a_2, \cdots, a_M$ をそれぞれ $p_1, p_2, \cdots, p_M$ で出力する無記憶情報源 $S$ の長さ $n$ の系列を考える。
この時，$n$ が十分大きいと，長さ $n$ に含まれる $a_i$ の出現回数 $n_i$ は，$n_i \approx n p_i$ となる。
- このような系列を **典型的系列 (代表的系列)** と呼ぶ

ある典型的系列 $\sigma$ が出現する確率は $\displaystyle P(\sigma) = \prod_{i=1}^M p_i^{n_i} \approx \prod_{i=1}^M 2^{n p_i \log_2 p_i} = 2^{-n H(S)}$ となる。
また，典型的系列以外の発生確率は $0$ となるので，典型的系列の数は $2^{n H(S)}$ となる。

:::note[典型的系列]
- 系列の長さ $n$ を十分大きく取った時に出現しうる系列を **典型的系列** と呼ぶ
- 典型的系列の出現確率は $2^{-n H(S)}$ であり，典型的系列の数は $2^{n H(S)}$ である
:::

### 数え上げ符号化
$2$ 元無記憶定常情報源の長さ $n$ の系列は，$1$ の個数 $i$ と，$1$ の個数が $i$ 個の系列全体の何番目かを数え上げれば符号化できる。  
この時，平均符号長 $\displaystyle L_n(i) = \frac{1}{n}(\log_2  {}_n C_i + \log_2 n)$ となる。
- $n$ が十分大きければ¥ $L_n(i) \approx H(S)$ となる

### その他
#### 適応符号化 : 情報源系列を観測しながら情報源のモデルを更新していく方法

#### 辞書法 : 長さ $n$ の部分系列を辞書に登録し，辞書での番号を符号化する方法
- 理論的には典型的系列のみ保存すれば良い $\to$ $2^{n H(S)}$ 個保存して，長さ + 辞書の順番で符号化する。 ($\log_2 n + nH(S)$ 桁)
- 辞書法の発展として，LZ77 法や LZW 法がある