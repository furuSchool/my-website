---
title: 5. 画像処理 (画像 → 情報)
---

:::warning[目標]
画像から様々な情報を抽出する手法を色々知る。
:::

---
## 2 値画像処理
あまり使われないが，画像を白黒の二値に変換して処理する手法について説明する。データ量が限られる時とかに使う。

### 二値化
画像を白黒の二値に変換する処理。
- p-tile法：画素値の分布を見て，閾値を決める
- **大津のしきい値法（判別分析法）**：クラス間分散を最大にする閾値を求める

:::tip[大津のしきい値法]
分離度 = クラス間分散 $\sigma_b^2$ / クラス内分散 $\sigma_w^2$ が最大になるような閾値を求める方法。  
全平均 $m_t$，全分散 $\sigma_t^2$，黒領域の画素値の平均 $m_1$，属する画素の個数 $\omega_1$，分散 $\sigma_1^2$，
白領域の画素値の平均 $m_2$，属する画素の個数 $\omega_2$，分散 $\sigma_2^2$ とすると，
- クラス内分散：$\sigma_w^2 = \frac{\omega_1 \sigma_1^2 + \omega_2 \sigma_2^2}{\omega_1 + \omega_2}$
- クラス間分散：$\sigma_b^2 = \frac{\omega_1 (m_1 - m_t)^2 + \omega_2 (m_2 - m_t)^2}{\omega_1 + \omega_2} 
= \frac{\omega_1 \omega_2 (m_1 - m_2)^2}{(\omega_1 + \omega_2)^2}$

また，全分散 $\sigma_t^2 = \sigma_w^2 + \sigma_b^2$ であるので，$\frac{\sigma_b^2}{\sigma_w^2} = \frac{\sigma_b^2}{\sigma_t^2 - \sigma_b^2}$ 
より， $\sigma_b^2$ を最大化すれば良い。
:::

### 連結性
：二つの領域が繋がっているか，独立なのか。
- 4 連結で定義したら，穴・背景は 8 連結で定義：対象は斜めは繋がっていない，穴・背景は繋がっている
- 8 連結で定義したら，穴・背景は 4 連結で定義：対象は斜めは繋がっている，穴・背景は斜めは繋がっていない
- ラベリング：同じ連結成分ごとにラベルをつける
- 参考になる資料：[画像処理　【連結成分の数え方】](https://note.com/branch_it_sol205/n/n63cd724fa76f)

### 輪郭追跡
：物体の連結部分の境界を求めること

### 膨張・収縮
：ごましおノイズ的なものを除去する方法例で，以下を繰り返す。最近は CNN や Graph-Cut など MRF 最適化を使う
- Dilation: 周辺の 1 ピクセル領域に拡大 (穴埋め)：クロージング。小さな穴を埋める
- Erosion: 外周の 1 ピクセルを侵食 (除去)：オープニング。小さな連結成分を除去

### 細線化
：太い線から細い線を取り出す。イメージは，「端点でない，連結性が保存される，境界上の」黒画素を白画素に変える。
- 参考になる資料：[細線化](https://imagingsolution.blog.fc2.com/blog-entry-138.html)

---
## 領域処理
:::warning[目標]
人間が視覚的に区別できるような領域の違いについて，その領域を抽出する手法を学ぶ。
:::

### テクスチャ
：形状や模様などの特徴量で，画像の分類・認識などに利用される。
- CNN はテクスチャを重点的に見ているとされ，Adversarial Attack もテクスチャに混ぜ込まれることが多い

#### フーリエ変換：テクスチャの特徴を周波数領域でみる。空間情報は残らない
- 全体におけるテクスチャの方向や周期が検出できる

#### ガボールフィルタ
ガボールフィルタ（正弦波とガウス関数の積）のフィルタバンクを使って，それぞれのフィルタから特徴を抽出する
- 局所的な特徴がいっぱいでき，それを貼り合わせれば全体の特徴がわかる
- 空間情報も残るため，全部のエッジを抽出できる

### 領域分割
- 身近な例：[SAM](https://segment-anything.com/)。画像を入力として，物体の領域を出力する画像セグメンテーションモデル
- k-means法：分割数 k を事前に指定し，RGB空間を使ってクラスタリングする
- mean-shift法：データ点を，点群が作る確率密度の極大点に向けて移動させる
    - 同じ領域を同じ色で塗りつぶすことで，Un-photorealistic rendering (写真$\rightarrow$アニメ風) などに使われる
    - クラスタリング，セグメンテーション，トラッキング，フィルタリングなどにも使われる
:::tip[k-means法]
1. 初期点 k 個
2. 一番近い点に分類（距離を定義しておく）
3. 点 k 個をそれぞれに属する点の重心に移動
4. 2と3を繰り返す
:::

---
## 特徴の検出
### テンプレートマッチング
：向きや大きさが決まった物体を画像中から探す方法。テンプレートを移動させて類似度を計算するが，とにかく演算コストが高い

- 計算の高速化の例
    - 解像度を変えながら階層的に処理する。ピクセル数を減らしたものからマッチングを行い，徐々に解像度を上げていく
    - アクティブ探索法。明らかマッチングしなかったらその周りは探索しない
- 類似度(距離)：画素値の差の二乗和，画素値の差の絶対値和，  
NCC（正規化相互相関，二つのベクトルの内積の正規化 = $\cos \theta$）など
- 直積量子化：距離計算の高速化例
    - ベクトルをいくつかの低次元ベクトルに分割してそれぞれを量子化（ = クラスタリング）。結果として，量子化された値の直積で表現される
    $\rightarrow$ あらかじめ作った量子化テーブルを使って，(近似)距離計算を行う
- 線画などでは，テンプレートと多少ずれていても大丈夫なように，細線化と逆のようなことを行う
- 類似検索は画像に限らず盛んに研究されている

### ハリスのコーナー検出
フィルタにより微分をとってハリス行列を求め，その固有値を使ってコーナーを検出する。  
画像は [Harrisコーナー検出器 (Harris Corner Detector)](https://cvml-expertguide.net/terms/cv/image-feature-detection/harris-corner-detector/) より引用。

<img src="/img/information/harris.png" style={{ width: '50%' }} />

- ハリス行列：$ \begin{pmatrix} I_x^2 & I_x I_y \\ I_x I_y & I_y^2 \end{pmatrix},~ I_x, I_y $ は画像の微分
- ハリス行列の成分は画素値周辺をガウシアンで重み付けして周りの微分値を取り込むこともある

### 機械学習的なコーナー検出：FAST アルゴリズム
：注目点と周辺の画素値の関係を記述して，ML（機械学習モデル）によってコーナー判定

### LoG や DoG によるコーナー検出
DoG で $\sigma$（ガウシアンの分散）を変えてフィルタリングした画像を複数用意。
周りのピクセル 8 つと分散が近い前後の 2 枚の画像のピクセル 9 × 2 つと比較して，極値を候補とする。プラスで多少の処理は必要。

### 特徴点の記述方法
例えば，特徴点を用いて画像間でマッチングする場合は，特徴点をベクトルなどで記述する必要がある。
- SIFT特徴量：特徴点周辺の勾配方向ヒストグラムを 128 次元のベクトルで表現
- バイナリ特徴量：特徴点周辺の画素値を 0 か 1 で表現してベクトル化

### 直線検出（ハフ変換）
:::tip[ハフ変換]
直線や円など基本的な形状を画像から抽出する手法。あらかじめ，エッジ抽出などで特徴点を抽出しておく。
- 直線検出：画像上の直線の方程式 $y = ax + b$（ $(x,y)$ は画像上の点） がパラメータ空間 $(a,b)$ だと $ b = - xa + y $ なのを用いて，
画像上の特徴点 $(x,y)$ による直線をパラメータ空間上で引きまくり，交差する点が多い点 $(a,b)$ をパラメータとする直線を検出する
- ただし，上記だとパラメータ空間が無限に広がるので，画像上の直線の方程式を $x \cos \theta + y \sin \theta = \rho$ として，
パラメータ空間を $(\theta, \rho)$ にする。$\rightarrow$ パラメータ空間上にいっぱい引かれた正弦波の交点が多い点を検出する
:::
- 直線だけではない場合，一般化ハフ変換を使う

---
## パターン認識
:::warning[目標]
あらかじめ定めたクラスに，画像を分類する手法について学ぶ。
:::

### パターン認識の流れ
:::tip[パターン認識の流れ]
1. 機械学習を用いない場合  
    1. データから特徴量を抽出 = **人間が考えた手法で**
    2. 特徴量を用いてクラス辞書を作成
    3. クラス辞書を用いて新しいデータを分類 $\rightarrow$ クラスの確率（距離）を出力
2. 機械学習を用いる場合
    1. データから特徴量を抽出して，識別モデルを機械学習で作成する = **特徴量抽出も自動的に**
    2. 識別モデルを用いて新しいデータを分類 $\rightarrow$ クラスの確率を出力
:::
- 機械学習は人間の思い込みを排除できる。ただし，データのバイアスには影響を受ける
- マハラノビス距離：データの分散を考慮した距離。正規化した距離というイメージ
    - 分散が大きいデータほど，その方向に対して距離はユークリッド距離と比べて短く考える

機械学習によるパターン認識には様々な方法があり，画像に限らず使われる。

### 1 つのデータをどのクラスに分類するか
- NN(Nearest Neighbor Search，最近傍探索)：最も近いデータと同じクラスに分類する
- KNN(K-Nearest Neighbor Search，K近傍法)：最も近い K 個のデータの多数決で分類する
    - NN と KNN は全ての点と距離を計算する必要があり，計算量が多い $\rightarrow$  Approximate NN（近似最近某探索），機械学習を用いた汎化，などがある
- 線形判別分析(LDA)：クラス間分散を最大化，クラス内分散を最小化するように分類する

### 教師なし学習でどう画像の分類を行うか
- k-means法もその一つ（領域分割で説明済み）
- **主成分分析(PCA)**：低次元の特徴空間を求める手法

:::tip[主成分分析(PCA)]
できるだけ情報を失わずに，データを低次元に圧縮する手法。  
- $N$ 個の $K$次元データ $\mathbf{x}_1 = (x_{11}, x_{12}, \ldots, x_{1K}), \ldots, \mathbf{x}_N = (x_{N1}, x_{N2}, \ldots, x_{NK})$
- 平均ベクトル $\mathbf{M} = \frac{1}{N} \sum_{i=1}^N \mathbf{x}_i$，
- 分散共分散行列 $\mathbf{S} = \frac{1}{N} \sum_{n=1}^N (\mathbf{x}_n - \mathbf{M}) (\mathbf{x}_n - \mathbf{M})^T$ 

$\mathbf{S}$ の固有ベクトルを求めて，固有値の大きい順に取り出した固有ベクトルで貼られる低次元空間の射影データを用いる。
:::
- 主成分分析で選ばれた基底と線形判別分析で選ばれた基底は直交する

### 教師あり学習でどう識別モデルを作るか
- 情報利得最大化
    - 分類前後で，エントロピー（情報量の不確かさ）が最も減るように分類する
    - 二次元では，平面上に垂直線か水平線を一本引いて分類するイメージ
- 決定木：情報利得最大化を行いながら分類を段階的に行う
    - 二次元では，平面上に垂直線か水平線を繰り返し引いて分類するイメージ
    - 学習データに敏感で，過学習が起きやすい（ = 汎化性能が低い）
- **Random forest**：データをランダムにサンプリングして決定木を複数作る & 説明変数をランダムに選んで決定木を複数作る $\rightarrow$ 多数決で最終的な分類を行う
    - 説明変数が多数，説明変数が欠損，などの状況にも対応できる
    - 説明変数の重要度（寄与度）を計算できる
- **SVM**(support vector machine，マージン最大化)：分離平面を引いて，その周りに最も近い点（サポートベクトル）との距離（マージン）を最大化する
    - 分離平面：$\mathbf{w} \cdot \mathbf{x} + b = 0$，
    目標：$\min_{\mathbf{w},b} \frac{1}{2} \|\mathbf{w}\|^2~\text{ s.t. }~y_i (\mathbf{w} \cdot \mathbf{x}_i + b) \geq 1$
        - 直線と点の距離は，$\frac{y_i (\mathbf{w} \cdot \mathbf{x}_i + b)}{\|\mathbf{w}\|}$ だよ
    - 平面決定に関わるのはサポートベクトル：$y_i (\mathbf{w} \cdot \mathbf{x}_i + b) = 1$ となる点だけ
    - マージン最大化は数学的には最適な戦略（テストエラーには上限値があり，マージン最大化のとき上限値を最小化する）
    - ソフトマージン SVM：マージン内に誤分類を許容する。
        - 目標：$\min_{\mathbf{w},b,\xi} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_i \xi_i~\text{ s.t. }~y_i (\mathbf{w} \cdot \mathbf{x}_i + b) \geq 1 - \xi_i,~\xi_i \geq 0$
    - カーネルトリック：非線形分離を可能にする
        - 分離平面：$\mathbf{w} \cdot \mathbf{\phi}(\mathbf{x}) + b = 0$，
        目標：$\min_{\mathbf{w},b} \frac{1}{2} \|\mathbf{w}\|^2~\text{ s.t. }~y_i (\mathbf{w} \cdot \mathbf{\phi}(\mathbf{x}_i) + b) \geq 1$
    - NN(ニューラルネットワーク)も，最適な超平面を引くことが目標ではある。ランダムに分離平面を引き，バックプロパゲーション(誤差逆伝播)で更新する
- LightGBM
    - 3 つの組み合わせ：決定木，アンサンブル学習，勾配ブースティング
    - 勾配ブースティング：単純多数決ではなく，前の学習器の結果を次の学習データに反映

---
## 深層学習による画像認識と生成

### DNN の特徴
:::tip[DNN の特徴]
特徴表現手法も学習で獲得できる！特徴抽出を手動で設計しなくても良い。
:::
- 逐次学習である $\rightarrow$ データの増加に対応しやすい ($\leftrightarrow$ SVM：batch learning )
- Pre-train (事前学習) を行ったモデルを用いて，新規ドメインのデータの学習を行う (Fine-tune, 転移学習の一種)
    - 画像なら，画像一般的な特徴の抽出を Pre-train $\rightarrow$ 特定領域のトレーニングを行う
- Branching (分岐) と Merging (Unification，統合) が簡単にできる

#### DNN の実現要因
1. データを大量に手に入れられるようになった
    - ラベリングも簡単にできるように
2. ムーアの法則に従い計算機の性能が向上することで，大規模なモデルを学習できるようになった

#### 汎化性能向上の工夫
- 活性化関数の工夫：シグモイド関数 $\rightarrow$ Rectified Linear Unit (ReLU)：勾配消失を防ぐ
- Dropout：ランダムに 50% 程度のユニットを無効にして バックプロパゲーションする。過学習を防ぐ効果がある
- Data Augmentation（データ拡張）：拡大縮小，反転，回転，色味変換，ランダム切り取りなど
    - 少ないトレーニングデータを水増する効果がある
    - 生成画像を使うと，精度がおかしくなることもある

### Perceptron（パーセプトロン）
：入力 $x$ に対して，出力 $y = u(\mathbf{w} \cdot \mathbf{x})$ を返すモデル。
- やっていることは，SVM とほとんど同じ。重み $\mathbf{w}$ を決定して平面を引いて，調整することを繰り返す
- 数学的には，誤差 $E = \frac{1}{2} (y - t)^2$ （ $t$： 教師データの分類結果）を最小化するよう $\mathbf{w}$ に
$\mathbf{w} - \rho \nabla E$ を代入して，誤差 $E$ を最小化する $\mathbf{w}$ を導く

### MLP（多層パーセプトロン）
：複数のパーセプトロンを積み重ねたモデル。隠れ層を持つことで，非線形分離が可能になる
- これが NN と言われるものの基本形
- バックプロパゲーション(BP，誤差逆伝播)によって，最適化を行う
- 深すぎると，勾配消失・局所最適化といった問題が発生する

### CNN（畳み込みニューラルネットワーク）
：畳み込み（画像の特徴抽出）とプーリング（代表値の抽出）を繰り返して，画像認識を行うモデル。
- 例えば，「32 × 32 の画像」$\rightarrow$「 6 枚の 5 × 5 のフィルタを通して 6 枚の 28 × 28 の特徴マップ」$\rightarrow$
「 2 × 2 のプーリングで 6 枚の 14 × 14 の画像」$\rightarrow$「...」$\rightarrow$ 全結合層（これまで得られた特徴マップから，クラスの確率の出力が得られる）
- 畳み込み：いわゆる，フィルタに似たパターン探し
- プーリング：近くの画素をまとめて，代表値を取る。位置の変化に対してロバストな特徴を得る
- この二つの処理が，CNN の特徴でありかつ，Adversarial Attack (敵対的画像生成) を可能にする原因

#### 画像認識：分類，検出，セグメンテーションなど。
- 分類：AlexNet，ResNet など。学習により，重みフィルタが，エッジ，カラー，テクスチャ，形状などを抽出するようなフィルタになる
- 物体検出
    - Region Proposal系：物体らしい領域を見つけ，領域の分類を行う。直観的で応用が広いが，リアルタイムに向かない。Fast R-CNN や Faster R-CNN など
    - Single-Shot系：グリッド状に分けて，各グリッドに物体があるかどうかを判定し，領域の精緻化を行って出力する。高速。YOLO や SSDなど
- セグメンテーション：画像中の物体の領域を画素単位で物体カテゴリに分類する。SegNet や U-Net など

### GAN（敵対的生成ネットワーク）
：生成器と識別器を競わせることで，本物っぽい画像を生成するモデル。
- 生成器：ノイズから，本物っぽい画像 (学習サンプルと同様な画像) を作る $\rightarrow$ LOSS を最小にするように頑張る
- 識別器：本物 (学習サンプル) を学習して、本物か偽物か (生成器が作った画像) を識別する
- 応用例：衛生写真 $\rightarrow$ 地図、白黒 $\rightarrow$ カラー など

### 拡散モデル (Diffusion Model)
：画像生成モデルの一つ。入力画像にノイズを加えて，ノイズを減らすことで入力と同じ画像を生成することで学習する。すなわち，ノイズ除去の方法を学ぶ。
- 特に自然な画像を生成しやすい，多様性がある，計算コストが高いかも
- 途中でプロンプトを入れることで，自由な画像生成が可能

---
## 動画像処理
### 差分画像
：異なる時間の画像の差をとった画像 ( = 差分画像) から物体を検出する。
- 背景差分法：背景のみの画像と，物体が映った画像の差分を取る
    - 純粋な背景画像を作れるか？
    - 背景が動いているときはどうするか？（日照，風の影響など）
- フレーム間差分法：連続する 3 枚の画像の差分画像をとり，それを AND 演算して移動物体の領域を得る
    - 単一色のものが動く場合うまくいかない（多少の動きでは差分がない）
- 統計的差分画像：画像の画素ごとに，前景か背景かを判別する
    - **ベイズの定理**を使う。
    - 画素値が $I$ の時，画素が $\omega_i$ に属する確率 $P(\omega_i|I) = \frac{P(I|\omega_i)P(\omega_i)}{P(I)}$ が最大になる $\omega_i$ を求める
        - $P(I|\omega_i)$：過去に得られた $\omega_i$ の画素値の分布 = 出現頻度により事前にわかる
        - $P(\omega_i)$：$\omega_i$ と分類される確率 = これも事前にわかる

### オプティカルフロー
：画像間の対象物体の動きや見え方の変化を二次元ベクトルで表現する手法。動物体検出のほかに **背景の動き = カメラの動き** を検出するのにも使われる
- ブロックマッチング法：時間方向にテンプレートマッチングを使い，対応点の変化を二次元ベクトルで表現する
    - テンプレートマッチング：テンプレートを画像上に移動させて類似度を計算する方法
- 勾配法：対象物体の動きが微小であることを前提にして，画像の勾配を求める
    - オプティカルフローの拘束条件式：$I_x$ と $I_y$ を画像の勾配，$I_t$ を時間方向の勾配，$u$ と $v$ をオプティカルフローとすると，
    $I_x u + I_y v + I_t = 0$
    - Lucas-Kanade法：注目画素近傍で動きは滑らかという仮定から，$m \times m$ のウィンドウ内で最小二乗法を用いてオプティカルフローを求める

### 物体追跡
：物体が次のフレームでどこに移動したかを見つける手法。
- テンプレートマッチング：「物体検出 $\rightarrow$ 次のフレームはその周辺でテンプレートマッチング」を繰り返す。計算コストがネック
- KLTトラッカー：特徴点の検出 + LK法によるオプティカルフロー
- Mean shift法：領域分割(特にカラーセグメンテーション)でも用いた mean-shift法を用いる
    - あらかじめ対象物体範囲を指定して色ヒストグラムを作成し，フレームごとにその周辺で類似度を評価する。
    その際，前の物体領域と現在の画像の色の類似度を高める方向に徐々に移動させ，極大となったところを物体の位置とする ( = Mean shift法)
    - 物体領域が全く重ならないくらい速い動きの場合は難しい
- ベイジアンフィルタ：物体の動きを確率的にモデル化する
    - 似た色の物体が重なってしまった時も，上記達の手法と違って物体を見失うことが少ない
    - 物体位置を中心としてランダムウォークなどで予測点を多数取る $\rightarrow$ 観測により点達の重み付けを行う 
    $\rightarrow$ 重みの高い点を残して再度サンプリング(リサンプリング)
    - 現状態は一つ前の状態にのみ依存する
